{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics  import   accuracy_score \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_teste(predictions, alg_name):\n",
    "\n",
    "    print('Resultados para o classificador {0}:'.format(alg_name))\n",
    "    print(classification_report(y_teste, predictions), \n",
    "    print (\"Acurácia para o treino é \", accuracy_score(y_teste,predictions)))\n",
    "    \n",
    "def report_treino(predictions, alg_name):\n",
    "\n",
    "    print('Resultados para o classificador {0}:'.format(alg_name))\n",
    "    print(classification_report(y_treino, predictions), \n",
    "    print (\"Acurácia para o treino é \", accuracy_score(y_treino,predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:\\\\Users\\\\Fabiel Fernando\\\\Desktop\\\\PROVA\\\\classificacao_Q4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando a existência de missings\n",
    "#dataset.apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x50</th>\n",
       "      <th>x51</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "      <th>x62</th>\n",
       "      <th>x63</th>\n",
       "      <th>x64</th>\n",
       "      <th>x65</th>\n",
       "      <th>x66</th>\n",
       "      <th>x67</th>\n",
       "      <th>x68</th>\n",
       "      <th>x69</th>\n",
       "      <th>x70</th>\n",
       "      <th>x71</th>\n",
       "      <th>x72</th>\n",
       "      <th>x73</th>\n",
       "      <th>x74</th>\n",
       "      <th>x75</th>\n",
       "      <th>x76</th>\n",
       "      <th>x77</th>\n",
       "      <th>x78</th>\n",
       "      <th>x79</th>\n",
       "      <th>x80</th>\n",
       "      <th>x81</th>\n",
       "      <th>x82</th>\n",
       "      <th>x83</th>\n",
       "      <th>x84</th>\n",
       "      <th>x85</th>\n",
       "      <th>x86</th>\n",
       "      <th>x87</th>\n",
       "      <th>x88</th>\n",
       "      <th>x89</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.696199</td>\n",
       "      <td>-0.792598</td>\n",
       "      <td>-0.349427</td>\n",
       "      <td>-0.464560</td>\n",
       "      <td>3.187014</td>\n",
       "      <td>0.035976</td>\n",
       "      <td>1.033274</td>\n",
       "      <td>-1.504968</td>\n",
       "      <td>0.204693</td>\n",
       "      <td>1.691204</td>\n",
       "      <td>-0.148668</td>\n",
       "      <td>-4.074097</td>\n",
       "      <td>-0.032896</td>\n",
       "      <td>-0.663494</td>\n",
       "      <td>-0.386016</td>\n",
       "      <td>-0.237805</td>\n",
       "      <td>-1.510523</td>\n",
       "      <td>-1.570864</td>\n",
       "      <td>-0.368605</td>\n",
       "      <td>0.812503</td>\n",
       "      <td>0.549905</td>\n",
       "      <td>-0.730260</td>\n",
       "      <td>0.761423</td>\n",
       "      <td>1.128273</td>\n",
       "      <td>-1.763750</td>\n",
       "      <td>0.579692</td>\n",
       "      <td>-0.293674</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>-0.427231</td>\n",
       "      <td>-0.295434</td>\n",
       "      <td>-2.626552</td>\n",
       "      <td>-0.888908</td>\n",
       "      <td>0.360110</td>\n",
       "      <td>-3.085644</td>\n",
       "      <td>-0.945316</td>\n",
       "      <td>-0.904486</td>\n",
       "      <td>1.072223</td>\n",
       "      <td>1.778115</td>\n",
       "      <td>-0.148051</td>\n",
       "      <td>0.634574</td>\n",
       "      <td>0.209628</td>\n",
       "      <td>0.561244</td>\n",
       "      <td>-0.586968</td>\n",
       "      <td>-3.702351</td>\n",
       "      <td>-0.649087</td>\n",
       "      <td>0.066648</td>\n",
       "      <td>0.521637</td>\n",
       "      <td>-0.318873</td>\n",
       "      <td>-0.964632</td>\n",
       "      <td>-0.068293</td>\n",
       "      <td>-1.941717</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>-0.030974</td>\n",
       "      <td>1.666534</td>\n",
       "      <td>1.907174</td>\n",
       "      <td>0.454065</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>-1.415378</td>\n",
       "      <td>-0.220428</td>\n",
       "      <td>-1.163591</td>\n",
       "      <td>0.643701</td>\n",
       "      <td>-0.593975</td>\n",
       "      <td>-0.230020</td>\n",
       "      <td>2.142668</td>\n",
       "      <td>-1.150896</td>\n",
       "      <td>1.980677</td>\n",
       "      <td>1.115755</td>\n",
       "      <td>0.511176</td>\n",
       "      <td>-0.526043</td>\n",
       "      <td>-0.492225</td>\n",
       "      <td>1.291322</td>\n",
       "      <td>-0.795223</td>\n",
       "      <td>1.292448</td>\n",
       "      <td>0.804562</td>\n",
       "      <td>0.822480</td>\n",
       "      <td>-1.205006</td>\n",
       "      <td>-0.280887</td>\n",
       "      <td>-1.364098</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>-1.925461</td>\n",
       "      <td>0.498012</td>\n",
       "      <td>0.371394</td>\n",
       "      <td>0.176175</td>\n",
       "      <td>0.547430</td>\n",
       "      <td>1.058247</td>\n",
       "      <td>0.503351</td>\n",
       "      <td>1.018997</td>\n",
       "      <td>0.221213</td>\n",
       "      <td>-0.419000</td>\n",
       "      <td>-0.858737</td>\n",
       "      <td>-0.534360</td>\n",
       "      <td>1.488142</td>\n",
       "      <td>-0.686337</td>\n",
       "      <td>2.084970</td>\n",
       "      <td>-0.685140</td>\n",
       "      <td>-2.049451</td>\n",
       "      <td>2.015426</td>\n",
       "      <td>1.158477</td>\n",
       "      <td>-0.309441</td>\n",
       "      <td>-1.549833</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.236696</td>\n",
       "      <td>-2.202342</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>1.497700</td>\n",
       "      <td>-0.069758</td>\n",
       "      <td>-2.467088</td>\n",
       "      <td>1.126529</td>\n",
       "      <td>-0.570557</td>\n",
       "      <td>2.079251</td>\n",
       "      <td>-1.882632</td>\n",
       "      <td>-0.827576</td>\n",
       "      <td>1.005103</td>\n",
       "      <td>-0.137394</td>\n",
       "      <td>1.189628</td>\n",
       "      <td>-0.851586</td>\n",
       "      <td>-1.288871</td>\n",
       "      <td>-0.963559</td>\n",
       "      <td>1.227582</td>\n",
       "      <td>0.715197</td>\n",
       "      <td>0.520097</td>\n",
       "      <td>0.588903</td>\n",
       "      <td>-0.590111</td>\n",
       "      <td>-2.210356</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>-1.039452</td>\n",
       "      <td>-0.241972</td>\n",
       "      <td>0.282824</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>-1.621286</td>\n",
       "      <td>-1.815760</td>\n",
       "      <td>0.663234</td>\n",
       "      <td>-0.208910</td>\n",
       "      <td>0.113045</td>\n",
       "      <td>2.046566</td>\n",
       "      <td>0.761385</td>\n",
       "      <td>1.412045</td>\n",
       "      <td>2.094611</td>\n",
       "      <td>-0.286475</td>\n",
       "      <td>0.718189</td>\n",
       "      <td>-0.421027</td>\n",
       "      <td>1.182153</td>\n",
       "      <td>0.379603</td>\n",
       "      <td>-0.835262</td>\n",
       "      <td>0.937721</td>\n",
       "      <td>0.114378</td>\n",
       "      <td>-0.651730</td>\n",
       "      <td>-0.047160</td>\n",
       "      <td>3.589095</td>\n",
       "      <td>-0.486826</td>\n",
       "      <td>2.847869</td>\n",
       "      <td>0.162564</td>\n",
       "      <td>-0.039426</td>\n",
       "      <td>0.462479</td>\n",
       "      <td>-1.531158</td>\n",
       "      <td>-1.860289</td>\n",
       "      <td>0.455750</td>\n",
       "      <td>2.220489</td>\n",
       "      <td>1.212844</td>\n",
       "      <td>-1.329690</td>\n",
       "      <td>-1.452428</td>\n",
       "      <td>0.053086</td>\n",
       "      <td>-0.574263</td>\n",
       "      <td>-2.518650</td>\n",
       "      <td>-1.737640</td>\n",
       "      <td>-0.194589</td>\n",
       "      <td>0.648973</td>\n",
       "      <td>-0.342163</td>\n",
       "      <td>-0.508209</td>\n",
       "      <td>0.947281</td>\n",
       "      <td>-0.430554</td>\n",
       "      <td>0.661217</td>\n",
       "      <td>-1.936414</td>\n",
       "      <td>-1.698198</td>\n",
       "      <td>-3.313671</td>\n",
       "      <td>-0.183713</td>\n",
       "      <td>-0.549041</td>\n",
       "      <td>1.280620</td>\n",
       "      <td>2.177973</td>\n",
       "      <td>0.706155</td>\n",
       "      <td>-1.002186</td>\n",
       "      <td>-0.760492</td>\n",
       "      <td>0.390230</td>\n",
       "      <td>1.652978</td>\n",
       "      <td>-0.281058</td>\n",
       "      <td>-2.274763</td>\n",
       "      <td>-1.451749</td>\n",
       "      <td>-0.594344</td>\n",
       "      <td>1.292452</td>\n",
       "      <td>1.066120</td>\n",
       "      <td>0.036062</td>\n",
       "      <td>0.498207</td>\n",
       "      <td>0.405567</td>\n",
       "      <td>0.509564</td>\n",
       "      <td>1.374071</td>\n",
       "      <td>-0.016943</td>\n",
       "      <td>-0.429280</td>\n",
       "      <td>-0.895016</td>\n",
       "      <td>1.259566</td>\n",
       "      <td>-0.354139</td>\n",
       "      <td>0.806797</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.436683</td>\n",
       "      <td>1.563816</td>\n",
       "      <td>-0.895999</td>\n",
       "      <td>-0.580425</td>\n",
       "      <td>0.311060</td>\n",
       "      <td>-0.187369</td>\n",
       "      <td>0.805249</td>\n",
       "      <td>-2.399522</td>\n",
       "      <td>-0.578818</td>\n",
       "      <td>1.586981</td>\n",
       "      <td>-1.941955</td>\n",
       "      <td>-0.596377</td>\n",
       "      <td>-0.489321</td>\n",
       "      <td>-1.030148</td>\n",
       "      <td>-0.485569</td>\n",
       "      <td>0.902347</td>\n",
       "      <td>0.107147</td>\n",
       "      <td>-0.780838</td>\n",
       "      <td>0.402332</td>\n",
       "      <td>-1.450170</td>\n",
       "      <td>-0.583627</td>\n",
       "      <td>-0.706544</td>\n",
       "      <td>-0.025883</td>\n",
       "      <td>-1.450107</td>\n",
       "      <td>2.118729</td>\n",
       "      <td>1.015845</td>\n",
       "      <td>0.166787</td>\n",
       "      <td>-0.044010</td>\n",
       "      <td>-0.360155</td>\n",
       "      <td>0.101155</td>\n",
       "      <td>-0.799201</td>\n",
       "      <td>-1.102617</td>\n",
       "      <td>2.115397</td>\n",
       "      <td>-2.361777</td>\n",
       "      <td>0.525674</td>\n",
       "      <td>-1.911165</td>\n",
       "      <td>0.123961</td>\n",
       "      <td>-0.417771</td>\n",
       "      <td>0.548105</td>\n",
       "      <td>-0.217684</td>\n",
       "      <td>-0.431924</td>\n",
       "      <td>-0.442644</td>\n",
       "      <td>-1.489144</td>\n",
       "      <td>-1.000744</td>\n",
       "      <td>0.862522</td>\n",
       "      <td>-0.563455</td>\n",
       "      <td>0.588636</td>\n",
       "      <td>0.010576</td>\n",
       "      <td>-0.456408</td>\n",
       "      <td>-1.428348</td>\n",
       "      <td>0.216525</td>\n",
       "      <td>1.290350</td>\n",
       "      <td>-1.092070</td>\n",
       "      <td>0.522418</td>\n",
       "      <td>2.553921</td>\n",
       "      <td>0.087687</td>\n",
       "      <td>1.755408</td>\n",
       "      <td>-1.382265</td>\n",
       "      <td>0.032006</td>\n",
       "      <td>0.680842</td>\n",
       "      <td>0.911192</td>\n",
       "      <td>0.505370</td>\n",
       "      <td>-0.741637</td>\n",
       "      <td>0.980315</td>\n",
       "      <td>2.359120</td>\n",
       "      <td>-0.380329</td>\n",
       "      <td>0.234811</td>\n",
       "      <td>2.287361</td>\n",
       "      <td>-0.568738</td>\n",
       "      <td>-1.932310</td>\n",
       "      <td>-1.912456</td>\n",
       "      <td>-1.829811</td>\n",
       "      <td>-0.589138</td>\n",
       "      <td>0.473086</td>\n",
       "      <td>-0.237060</td>\n",
       "      <td>-0.106093</td>\n",
       "      <td>-0.690060</td>\n",
       "      <td>-0.640960</td>\n",
       "      <td>-1.088658</td>\n",
       "      <td>-0.998397</td>\n",
       "      <td>-1.579437</td>\n",
       "      <td>-0.697638</td>\n",
       "      <td>-0.620487</td>\n",
       "      <td>-0.320028</td>\n",
       "      <td>1.390414</td>\n",
       "      <td>0.449638</td>\n",
       "      <td>0.300941</td>\n",
       "      <td>-0.512526</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.265771</td>\n",
       "      <td>-2.630024</td>\n",
       "      <td>0.933578</td>\n",
       "      <td>-1.285978</td>\n",
       "      <td>0.503162</td>\n",
       "      <td>0.204829</td>\n",
       "      <td>-0.753835</td>\n",
       "      <td>0.290033</td>\n",
       "      <td>1.721487</td>\n",
       "      <td>1.304518</td>\n",
       "      <td>0.478903</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.425908</td>\n",
       "      <td>0.400055</td>\n",
       "      <td>-0.305038</td>\n",
       "      <td>-0.930251</td>\n",
       "      <td>-2.214549</td>\n",
       "      <td>1.763379</td>\n",
       "      <td>-0.239868</td>\n",
       "      <td>-2.058891</td>\n",
       "      <td>-1.006533</td>\n",
       "      <td>-2.156839</td>\n",
       "      <td>-0.817310</td>\n",
       "      <td>3.135035</td>\n",
       "      <td>-1.046031</td>\n",
       "      <td>2.035231</td>\n",
       "      <td>0.307369</td>\n",
       "      <td>-0.831289</td>\n",
       "      <td>-0.263652</td>\n",
       "      <td>-1.479070</td>\n",
       "      <td>-0.675276</td>\n",
       "      <td>-0.222479</td>\n",
       "      <td>-0.441100</td>\n",
       "      <td>0.343649</td>\n",
       "      <td>0.210042</td>\n",
       "      <td>-2.030159</td>\n",
       "      <td>0.636847</td>\n",
       "      <td>-2.268783</td>\n",
       "      <td>1.066813</td>\n",
       "      <td>1.486655</td>\n",
       "      <td>0.665269</td>\n",
       "      <td>1.207031</td>\n",
       "      <td>3.549965</td>\n",
       "      <td>-0.026904</td>\n",
       "      <td>1.027441</td>\n",
       "      <td>1.979429</td>\n",
       "      <td>1.133188</td>\n",
       "      <td>1.709450</td>\n",
       "      <td>1.046510</td>\n",
       "      <td>1.397032</td>\n",
       "      <td>0.177327</td>\n",
       "      <td>-0.402179</td>\n",
       "      <td>-0.054244</td>\n",
       "      <td>-0.578126</td>\n",
       "      <td>-0.055127</td>\n",
       "      <td>2.794188</td>\n",
       "      <td>0.528181</td>\n",
       "      <td>-0.140851</td>\n",
       "      <td>-0.320488</td>\n",
       "      <td>-0.552952</td>\n",
       "      <td>-2.406692</td>\n",
       "      <td>0.054562</td>\n",
       "      <td>0.886823</td>\n",
       "      <td>-0.419061</td>\n",
       "      <td>-0.272393</td>\n",
       "      <td>-2.141239</td>\n",
       "      <td>-0.114749</td>\n",
       "      <td>0.230638</td>\n",
       "      <td>-0.250862</td>\n",
       "      <td>1.116209</td>\n",
       "      <td>1.452902</td>\n",
       "      <td>0.927677</td>\n",
       "      <td>-0.136729</td>\n",
       "      <td>-0.873607</td>\n",
       "      <td>0.430335</td>\n",
       "      <td>0.828970</td>\n",
       "      <td>0.313719</td>\n",
       "      <td>0.378332</td>\n",
       "      <td>-0.586515</td>\n",
       "      <td>-1.448876</td>\n",
       "      <td>-0.149765</td>\n",
       "      <td>-0.958114</td>\n",
       "      <td>-1.478115</td>\n",
       "      <td>-2.388252</td>\n",
       "      <td>-1.569214</td>\n",
       "      <td>-2.755844</td>\n",
       "      <td>-1.098166</td>\n",
       "      <td>1.450431</td>\n",
       "      <td>1.134263</td>\n",
       "      <td>2.586703</td>\n",
       "      <td>-0.224750</td>\n",
       "      <td>-0.036701</td>\n",
       "      <td>2.264622</td>\n",
       "      <td>-0.035200</td>\n",
       "      <td>0.217302</td>\n",
       "      <td>0.038805</td>\n",
       "      <td>-0.604043</td>\n",
       "      <td>-1.798876</td>\n",
       "      <td>-2.307973</td>\n",
       "      <td>1.441341</td>\n",
       "      <td>2.311820</td>\n",
       "      <td>-0.947016</td>\n",
       "      <td>-0.260665</td>\n",
       "      <td>-0.849927</td>\n",
       "      <td>1.402768</td>\n",
       "      <td>0.393653</td>\n",
       "      <td>-1.466818</td>\n",
       "      <td>0.152257</td>\n",
       "      <td>-4.004950</td>\n",
       "      <td>0.676342</td>\n",
       "      <td>-1.927319</td>\n",
       "      <td>1.959032</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.186156</td>\n",
       "      <td>-0.975764</td>\n",
       "      <td>0.594660</td>\n",
       "      <td>-1.181980</td>\n",
       "      <td>-1.443414</td>\n",
       "      <td>-0.797651</td>\n",
       "      <td>-1.252608</td>\n",
       "      <td>-0.060452</td>\n",
       "      <td>0.130702</td>\n",
       "      <td>-2.343517</td>\n",
       "      <td>0.892393</td>\n",
       "      <td>-0.533092</td>\n",
       "      <td>-0.760388</td>\n",
       "      <td>-0.702277</td>\n",
       "      <td>0.259456</td>\n",
       "      <td>3.732211</td>\n",
       "      <td>1.185647</td>\n",
       "      <td>2.046445</td>\n",
       "      <td>-1.378246</td>\n",
       "      <td>-0.733557</td>\n",
       "      <td>4.716702</td>\n",
       "      <td>0.229157</td>\n",
       "      <td>1.955133</td>\n",
       "      <td>1.917857</td>\n",
       "      <td>-1.783127</td>\n",
       "      <td>-0.839499</td>\n",
       "      <td>-1.811106</td>\n",
       "      <td>-0.405222</td>\n",
       "      <td>0.074332</td>\n",
       "      <td>2.034061</td>\n",
       "      <td>0.179220</td>\n",
       "      <td>-0.458617</td>\n",
       "      <td>-3.470883</td>\n",
       "      <td>0.561481</td>\n",
       "      <td>0.492969</td>\n",
       "      <td>1.310855</td>\n",
       "      <td>0.505790</td>\n",
       "      <td>-1.135986</td>\n",
       "      <td>-0.696156</td>\n",
       "      <td>0.815568</td>\n",
       "      <td>-0.266634</td>\n",
       "      <td>0.245124</td>\n",
       "      <td>1.244601</td>\n",
       "      <td>0.930504</td>\n",
       "      <td>-2.423524</td>\n",
       "      <td>-0.217978</td>\n",
       "      <td>-0.250712</td>\n",
       "      <td>-0.180181</td>\n",
       "      <td>1.579620</td>\n",
       "      <td>-1.239677</td>\n",
       "      <td>-0.917660</td>\n",
       "      <td>1.345773</td>\n",
       "      <td>0.545109</td>\n",
       "      <td>2.444263</td>\n",
       "      <td>-1.244190</td>\n",
       "      <td>0.446668</td>\n",
       "      <td>0.178714</td>\n",
       "      <td>-0.714363</td>\n",
       "      <td>0.310813</td>\n",
       "      <td>-4.723429</td>\n",
       "      <td>1.025380</td>\n",
       "      <td>0.567891</td>\n",
       "      <td>-1.215820</td>\n",
       "      <td>0.061255</td>\n",
       "      <td>1.798139</td>\n",
       "      <td>-0.254473</td>\n",
       "      <td>0.091907</td>\n",
       "      <td>0.680257</td>\n",
       "      <td>1.232538</td>\n",
       "      <td>-0.482364</td>\n",
       "      <td>1.012526</td>\n",
       "      <td>-0.554645</td>\n",
       "      <td>0.451229</td>\n",
       "      <td>0.484063</td>\n",
       "      <td>2.466720</td>\n",
       "      <td>0.102488</td>\n",
       "      <td>-0.574971</td>\n",
       "      <td>-2.885352</td>\n",
       "      <td>0.911710</td>\n",
       "      <td>-0.846603</td>\n",
       "      <td>0.850602</td>\n",
       "      <td>2.222440</td>\n",
       "      <td>-1.981894</td>\n",
       "      <td>0.156248</td>\n",
       "      <td>-2.788302</td>\n",
       "      <td>-0.067919</td>\n",
       "      <td>1.352606</td>\n",
       "      <td>-1.878879</td>\n",
       "      <td>-0.943184</td>\n",
       "      <td>-0.185896</td>\n",
       "      <td>1.098563</td>\n",
       "      <td>-1.444435</td>\n",
       "      <td>-1.818126</td>\n",
       "      <td>0.446574</td>\n",
       "      <td>0.239328</td>\n",
       "      <td>0.802939</td>\n",
       "      <td>-2.035289</td>\n",
       "      <td>-1.433793</td>\n",
       "      <td>-0.218596</td>\n",
       "      <td>0.619317</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0  1.696199 -0.792598 -0.349427 -0.464560  3.187014  0.035976  1.033274   \n",
       "1 -0.236696 -2.202342  0.024023  1.497700 -0.069758 -2.467088  1.126529   \n",
       "2 -0.436683  1.563816 -0.895999 -0.580425  0.311060 -0.187369  0.805249   \n",
       "3  1.425908  0.400055 -0.305038 -0.930251 -2.214549  1.763379 -0.239868   \n",
       "4 -0.186156 -0.975764  0.594660 -1.181980 -1.443414 -0.797651 -1.252608   \n",
       "\n",
       "         x7        x8        x9       x10       x11       x12       x13  \\\n",
       "0 -1.504968  0.204693  1.691204 -0.148668 -4.074097 -0.032896 -0.663494   \n",
       "1 -0.570557  2.079251 -1.882632 -0.827576  1.005103 -0.137394  1.189628   \n",
       "2 -2.399522 -0.578818  1.586981 -1.941955 -0.596377 -0.489321 -1.030148   \n",
       "3 -2.058891 -1.006533 -2.156839 -0.817310  3.135035 -1.046031  2.035231   \n",
       "4 -0.060452  0.130702 -2.343517  0.892393 -0.533092 -0.760388 -0.702277   \n",
       "\n",
       "        x14       x15       x16       x17       x18       x19       x20  \\\n",
       "0 -0.386016 -0.237805 -1.510523 -1.570864 -0.368605  0.812503  0.549905   \n",
       "1 -0.851586 -1.288871 -0.963559  1.227582  0.715197  0.520097  0.588903   \n",
       "2 -0.485569  0.902347  0.107147 -0.780838  0.402332 -1.450170 -0.583627   \n",
       "3  0.307369 -0.831289 -0.263652 -1.479070 -0.675276 -0.222479 -0.441100   \n",
       "4  0.259456  3.732211  1.185647  2.046445 -1.378246 -0.733557  4.716702   \n",
       "\n",
       "        x21       x22       x23       x24       x25       x26       x27  \\\n",
       "0 -0.730260  0.761423  1.128273 -1.763750  0.579692 -0.293674  0.295500   \n",
       "1 -0.590111 -2.210356  1.022461 -1.039452 -0.241972  0.282824  0.001147   \n",
       "2 -0.706544 -0.025883 -1.450107  2.118729  1.015845  0.166787 -0.044010   \n",
       "3  0.343649  0.210042 -2.030159  0.636847 -2.268783  1.066813  1.486655   \n",
       "4  0.229157  1.955133  1.917857 -1.783127 -0.839499 -1.811106 -0.405222   \n",
       "\n",
       "        x28       x29       x30       x31       x32       x33       x34  \\\n",
       "0 -0.427231 -0.295434 -2.626552 -0.888908  0.360110 -3.085644 -0.945316   \n",
       "1 -1.621286 -1.815760  0.663234 -0.208910  0.113045  2.046566  0.761385   \n",
       "2 -0.360155  0.101155 -0.799201 -1.102617  2.115397 -2.361777  0.525674   \n",
       "3  0.665269  1.207031  3.549965 -0.026904  1.027441  1.979429  1.133188   \n",
       "4  0.074332  2.034061  0.179220 -0.458617 -3.470883  0.561481  0.492969   \n",
       "\n",
       "        x35       x36       x37       x38       x39       x40       x41  \\\n",
       "0 -0.904486  1.072223  1.778115 -0.148051  0.634574  0.209628  0.561244   \n",
       "1  1.412045  2.094611 -0.286475  0.718189 -0.421027  1.182153  0.379603   \n",
       "2 -1.911165  0.123961 -0.417771  0.548105 -0.217684 -0.431924 -0.442644   \n",
       "3  1.709450  1.046510  1.397032  0.177327 -0.402179 -0.054244 -0.578126   \n",
       "4  1.310855  0.505790 -1.135986 -0.696156  0.815568 -0.266634  0.245124   \n",
       "\n",
       "        x42       x43       x44       x45       x46       x47       x48  \\\n",
       "0 -0.586968 -3.702351 -0.649087  0.066648  0.521637 -0.318873 -0.964632   \n",
       "1 -0.835262  0.937721  0.114378 -0.651730 -0.047160  3.589095 -0.486826   \n",
       "2 -1.489144 -1.000744  0.862522 -0.563455  0.588636  0.010576 -0.456408   \n",
       "3 -0.055127  2.794188  0.528181 -0.140851 -0.320488 -0.552952 -2.406692   \n",
       "4  1.244601  0.930504 -2.423524 -0.217978 -0.250712 -0.180181  1.579620   \n",
       "\n",
       "        x49       x50       x51       x52       x53       x54       x55  \\\n",
       "0 -0.068293 -1.941717  0.011300 -0.030974  1.666534  1.907174  0.454065   \n",
       "1  2.847869  0.162564 -0.039426  0.462479 -1.531158 -1.860289  0.455750   \n",
       "2 -1.428348  0.216525  1.290350 -1.092070  0.522418  2.553921  0.087687   \n",
       "3  0.054562  0.886823 -0.419061 -0.272393 -2.141239 -0.114749  0.230638   \n",
       "4 -1.239677 -0.917660  1.345773  0.545109  2.444263 -1.244190  0.446668   \n",
       "\n",
       "        x56       x57       x58       x59       x60       x61       x62  \\\n",
       "0  0.157899 -1.415378 -0.220428 -1.163591  0.643701 -0.593975 -0.230020   \n",
       "1  2.220489  1.212844 -1.329690 -1.452428  0.053086 -0.574263 -2.518650   \n",
       "2  1.755408 -1.382265  0.032006  0.680842  0.911192  0.505370 -0.741637   \n",
       "3 -0.250862  1.116209  1.452902  0.927677 -0.136729 -0.873607  0.430335   \n",
       "4  0.178714 -0.714363  0.310813 -4.723429  1.025380  0.567891 -1.215820   \n",
       "\n",
       "        x63       x64       x65       x66       x67       x68       x69  \\\n",
       "0  2.142668 -1.150896  1.980677  1.115755  0.511176 -0.526043 -0.492225   \n",
       "1 -1.737640 -0.194589  0.648973 -0.342163 -0.508209  0.947281 -0.430554   \n",
       "2  0.980315  2.359120 -0.380329  0.234811  2.287361 -0.568738 -1.932310   \n",
       "3  0.828970  0.313719  0.378332 -0.586515 -1.448876 -0.149765 -0.958114   \n",
       "4  0.061255  1.798139 -0.254473  0.091907  0.680257  1.232538 -0.482364   \n",
       "\n",
       "        x70       x71       x72       x73       x74       x75       x76  \\\n",
       "0  1.291322 -0.795223  1.292448  0.804562  0.822480 -1.205006 -0.280887   \n",
       "1  0.661217 -1.936414 -1.698198 -3.313671 -0.183713 -0.549041  1.280620   \n",
       "2 -1.912456 -1.829811 -0.589138  0.473086 -0.237060 -0.106093 -0.690060   \n",
       "3 -1.478115 -2.388252 -1.569214 -2.755844 -1.098166  1.450431  1.134263   \n",
       "4  1.012526 -0.554645  0.451229  0.484063  2.466720  0.102488 -0.574971   \n",
       "\n",
       "        x77       x78       x79       x80       x81       x82       x83  \\\n",
       "0 -1.364098  0.312000 -1.925461  0.498012  0.371394  0.176175  0.547430   \n",
       "1  2.177973  0.706155 -1.002186 -0.760492  0.390230  1.652978 -0.281058   \n",
       "2 -0.640960 -1.088658 -0.998397 -1.579437 -0.697638 -0.620487 -0.320028   \n",
       "3  2.586703 -0.224750 -0.036701  2.264622 -0.035200  0.217302  0.038805   \n",
       "4 -2.885352  0.911710 -0.846603  0.850602  2.222440 -1.981894  0.156248   \n",
       "\n",
       "        x84       x85       x86       x87       x88       x89       x90  \\\n",
       "0  1.058247  0.503351  1.018997  0.221213 -0.419000 -0.858737 -0.534360   \n",
       "1 -2.274763 -1.451749 -0.594344  1.292452  1.066120  0.036062  0.498207   \n",
       "2  1.390414  0.449638  0.300941 -0.512526  0.656667  0.265771 -2.630024   \n",
       "3 -0.604043 -1.798876 -2.307973  1.441341  2.311820 -0.947016 -0.260665   \n",
       "4 -2.788302 -0.067919  1.352606 -1.878879 -0.943184 -0.185896  1.098563   \n",
       "\n",
       "        x91       x92       x93       x94       x95       x96       x97  \\\n",
       "0  1.488142 -0.686337  2.084970 -0.685140 -2.049451  2.015426  1.158477   \n",
       "1  0.405567  0.509564  1.374071 -0.016943 -0.429280 -0.895016  1.259566   \n",
       "2  0.933578 -1.285978  0.503162  0.204829 -0.753835  0.290033  1.721487   \n",
       "3 -0.849927  1.402768  0.393653 -1.466818  0.152257 -4.004950  0.676342   \n",
       "4 -1.444435 -1.818126  0.446574  0.239328  0.802939 -2.035289 -1.433793   \n",
       "\n",
       "        x98       x99  target  \n",
       "0 -0.309441 -1.549833     4.0  \n",
       "1 -0.354139  0.806797     5.0  \n",
       "2  1.304518  0.478903     3.0  \n",
       "3 -1.927319  1.959032     8.0  \n",
       "4 -0.218596  0.619317     9.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos nossos dados:\n",
      " (1500, 101)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensão dos nossos dados:\\n\", \n",
    "     dataset.shape)\n",
    "#print(\"Tipo de variáveis:\\n\",\n",
    "#     dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pocentagem da variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># target</th>\n",
       "      <th>% target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>153</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>153</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>152</td>\n",
       "      <td>10.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>150</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>150</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>149</td>\n",
       "      <td>9.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>149</td>\n",
       "      <td>9.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>149</td>\n",
       "      <td>9.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>148</td>\n",
       "      <td>9.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>147</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     # target   % target\n",
       "2.0       153  10.200000\n",
       "8.0       153  10.200000\n",
       "3.0       152  10.133333\n",
       "0.0       150  10.000000\n",
       "4.0       150  10.000000\n",
       "1.0       149   9.933333\n",
       "6.0       149   9.933333\n",
       "9.0       149   9.933333\n",
       "5.0       148   9.866667\n",
       "7.0       147   9.800000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = dataset['target']\n",
    "count = pd.DataFrame(resposta.value_counts())\n",
    "percent = pd.DataFrame(resposta.value_counts(normalize = True)*100)\n",
    "table = pd.concat([count, percent], axis = 1)\n",
    "table.columns = ['# target', '% target']\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descritiva de algumas variáveis\n",
    "#dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space = dataset.iloc[:, dataset.columns != 'target']\n",
    "feature_class = dataset.iloc[:, dataset.columns == 'target']\n",
    "\n",
    "\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(feature_space,\n",
    "                                                                    feature_class,\n",
    "                                                                    test_size = 0.30, \n",
    "                                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar conjuntos de teste para evitar futuras mensagens de aviso\n",
    "y_treino = y_treino.values.ravel() \n",
    "y_teste = y_teste.values.ravel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustando Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = GaussianNB(priors=None)\n",
    "\n",
    "classifier.fit(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precisão do classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = classifier.predict(X_teste)\n",
    "pred_train = classifier.predict(X_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabela com cálculo de vária métricas conjunto treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador Naive Bayes:\n",
      "Acurácia para o treino é  0.6885714285714286\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.79      0.72       103\n",
      "        1.0       0.58      0.68      0.63        98\n",
      "        2.0       0.67      0.54      0.60       111\n",
      "        3.0       0.86      0.69      0.76       105\n",
      "        4.0       0.72      0.76      0.74       104\n",
      "        5.0       0.73      0.55      0.62        97\n",
      "        6.0       0.70      0.62      0.66        98\n",
      "        7.0       0.76      0.67      0.71       111\n",
      "        8.0       0.63      0.76      0.69       106\n",
      "        9.0       0.67      0.81      0.73       117\n",
      "\n",
      "avg / total       0.70      0.69      0.69      1050\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "report_treino(pred_train,'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabela com cálculo de vária métricas conjunto teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador Naive Bayes:\n",
      "Acurácia para o treino é  0.62\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.85      0.72        47\n",
      "        1.0       0.51      0.67      0.58        51\n",
      "        2.0       0.60      0.50      0.55        42\n",
      "        3.0       0.68      0.64      0.66        47\n",
      "        4.0       0.63      0.48      0.54        46\n",
      "        5.0       0.72      0.57      0.64        51\n",
      "        6.0       0.79      0.53      0.64        51\n",
      "        7.0       0.65      0.47      0.55        36\n",
      "        8.0       0.63      0.77      0.69        47\n",
      "        9.0       0.48      0.72      0.57        32\n",
      "\n",
      "avg / total       0.64      0.62      0.62       450\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "report_teste(pred_test,'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustando o classificador com Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_kfold = KFold(10, shuffle = False)\n",
    "\n",
    "param_grid = param_grid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_nb = GridSearchCV(fit_nb,\n",
    "                     cv = cv_kfold,\n",
    "                     param_grid = param_grid, \n",
    "                     n_jobs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n",
       "       error_score='raise', estimator=GaussianNB(priors=None),\n",
       "       fit_params=None, iid=True, n_jobs=3, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_nb.fit(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_nb.set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_nb.fit(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados Conjunto Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador Naive Bayes com Grid Search:\n",
      "Acurácia para o treino é  0.6885714285714286\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.79      0.72       103\n",
      "        1.0       0.58      0.68      0.63        98\n",
      "        2.0       0.67      0.54      0.60       111\n",
      "        3.0       0.86      0.69      0.76       105\n",
      "        4.0       0.72      0.76      0.74       104\n",
      "        5.0       0.73      0.55      0.62        97\n",
      "        6.0       0.70      0.62      0.66        98\n",
      "        7.0       0.76      0.67      0.71       111\n",
      "        8.0       0.63      0.76      0.69       106\n",
      "        9.0       0.67      0.81      0.73       117\n",
      "\n",
      "avg / total       0.70      0.69      0.69      1050\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "pred_train2 = fit_nb.predict(X_treino)\n",
    "report_treino(pred_train2, 'Naive Bayes com Grid Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados conjunto teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador Naive Bayes com Grid Search:\n",
      "Acurácia para o treino é  0.62\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.85      0.72        47\n",
      "        1.0       0.51      0.67      0.58        51\n",
      "        2.0       0.60      0.50      0.55        42\n",
      "        3.0       0.68      0.64      0.66        47\n",
      "        4.0       0.63      0.48      0.54        46\n",
      "        5.0       0.72      0.57      0.64        51\n",
      "        6.0       0.79      0.53      0.64        51\n",
      "        7.0       0.65      0.47      0.55        36\n",
      "        8.0       0.63      0.77      0.69        47\n",
      "        9.0       0.48      0.72      0.57        32\n",
      "\n",
      "avg / total       0.64      0.62      0.62       450\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "predictions_fit_nb = fit_nb.predict(X_teste)\n",
    "report_teste(predictions_fit_nb, 'Naive Bayes com Grid Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_nb = fit_nb.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40  0  0  0  0  3  0  1  3  0]\n",
      " [ 0 34  3  1  6  0  2  0  0  5]\n",
      " [ 4  5 21  4  1  4  1  1  0  1]\n",
      " [ 2  6  1 30  0  0  0  1  0  7]\n",
      " [ 1 10  0  4 22  1  0  0  0  8]\n",
      " [ 5  9  2  1  0 29  1  0  3  1]\n",
      " [ 0  0  6  0  3  1 27  4  8  2]\n",
      " [10  1  0  1  0  0  0 17  7  0]\n",
      " [ 1  0  2  0  2  0  3  2 36  1]\n",
      " [ 1  2  0  3  1  2  0  0  0 23]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_teste, predictions_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui está a nossa precisão média no conjunto de testes: 0.620\n"
     ]
    }
   ],
   "source": [
    "accuracy_nb = fit_nb.score(X_teste, y_teste)\n",
    "\n",
    "print(\"Aqui está a nossa precisão média no conjunto de testes: {0:.3f}\".format(accuracy_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de erro de teste para o nosso modelo é:  0.380\n"
     ]
    }
   ],
   "source": [
    "test_error_rate_nb = 1 - accuracy_nb\n",
    "print(\"A taxa de erro de teste para o nosso modelo é: {0: .3f}\" .format(test_error_rate_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_prob = fit_nb.predict_proba(X_teste)[:, 1]\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_teste,\n",
    "                          predictions_prob,\n",
    "                          pos_label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_nb = auc(fpr2, tpr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr2, tpr2, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % auc_nb)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falso Positivo')\n",
    "plt.ylabel('Taxa de Verdadeiro Positivo')\n",
    "plt.title('Receiver operating characteristic ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador Naive Bayes:\n",
      "Acurácia para o treino é  0.62\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.85      0.72        47\n",
      "        1.0       0.51      0.67      0.58        51\n",
      "        2.0       0.60      0.50      0.55        42\n",
      "        3.0       0.68      0.64      0.66        47\n",
      "        4.0       0.63      0.48      0.54        46\n",
      "        5.0       0.72      0.57      0.64        51\n",
      "        6.0       0.79      0.53      0.64        51\n",
      "        7.0       0.65      0.47      0.55        36\n",
      "        8.0       0.63      0.77      0.69        47\n",
      "        9.0       0.48      0.72      0.57        32\n",
      "\n",
      "avg / total       0.64      0.62      0.62       450\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "    report_teste(predictions_nb, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 0:100].values\n",
    "y = dataset['target'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6306666666666667, 0.042184778718817)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=42)\n",
    "model = GaussianNB()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "results.mean(), results.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.636"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "accuracies = cross_val_score(model, X=X, y=y, cv=LeaveOneOut())\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6313333333333333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv_repeat = RepeatedKFold(n_splits=6, n_repeats=3, random_state=42)\n",
    "model = GaussianNB()\n",
    "accuracies = cross_val_score(model, X=X, y=y, cv=cv_repeat)\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando as k primeiras observações para treino e o restante para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino = dataset.iloc[0:499, 0:99].values\n",
    "y_treino = dataset.iloc[0:499, 100].values\n",
    "\n",
    "\n",
    "X_teste = dataset.iloc[500:1500, 0:99].values\n",
    "y_teste = dataset.iloc[500:1500, 100].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precisão do classificador no Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_teste = clf.predict(X_teste)\n",
    "pred_treino = clf.predict(X_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador Naive Bayes:\n",
      "Acurácia para o treino é  0.751503006012024\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.80      0.74        55\n",
      "        1.0       0.62      0.75      0.68        48\n",
      "        2.0       0.71      0.60      0.65        45\n",
      "        3.0       0.85      0.76      0.80        58\n",
      "        4.0       0.81      0.88      0.84        48\n",
      "        5.0       0.80      0.61      0.69        46\n",
      "        6.0       0.73      0.70      0.72        47\n",
      "        7.0       0.88      0.71      0.78        51\n",
      "        8.0       0.70      0.90      0.79        41\n",
      "        9.0       0.79      0.80      0.79        60\n",
      "\n",
      "avg / total       0.76      0.75      0.75       499\n",
      " None\n"
     ]
    }
   ],
   "source": [
    " report_treino(pred_treino, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador Naive Bayes:\n",
      "Acurácia para o treino é  0.607\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      0.81      0.68        95\n",
      "        1.0       0.52      0.55      0.54       101\n",
      "        2.0       0.53      0.47      0.50       108\n",
      "        3.0       0.65      0.65      0.65        94\n",
      "        4.0       0.65      0.52      0.58       102\n",
      "        5.0       0.67      0.49      0.56       102\n",
      "        6.0       0.59      0.61      0.60       102\n",
      "        7.0       0.68      0.52      0.59        96\n",
      "        8.0       0.66      0.68      0.67       111\n",
      "        9.0       0.59      0.80      0.68        89\n",
      "\n",
      "avg / total       0.61      0.61      0.60      1000\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "report_teste(pred_teste, 'Naive Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
